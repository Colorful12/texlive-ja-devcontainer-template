%% example for producing articles in MVA format using LaTeX.
%% written by Takeshi MASUDA, Electrotechnical Laboratory, Japan in May 1996.
%% modified by KAGESAWA Masataka, OKAZAKI Shin'ichro, YASUMOTO Mamoru.
%% last modified by Masaki Onishi, AIST, in Nov 2012.
%% use at your own risk.

\documentclass{mva_style}
\usepackage{graphicx}
\usepackage{amsmath,amssymb, yhmath,bm} % for expectation sign


\finalcopy %Uncomment this line for the Camera-Ready Manuscript

\begin{document}
\title{Brush Style Transfer with Feedforward Neural Networks}

\author{
  Saki Kohata\\
  Waseda University\\
  Tokyo\\
  {\tt cat-73@akane.waseda.jp}\\
  \and
  Edgar Simo-Serra\\
  Waseda University\\
  Tokyo\\
  {\tt ess@waseda.jp}\\
}

\maketitle

\section*{\centering Abstract}
\textit{
  The emergence of machine learning has revolutionized the world of digital 
  art, providing people with the ability to create highly polished images 
  without advanced drawing skills. 
  However, most of the studies that have been conducted to generate pictures 
  using machine learning are not tools that can be used by people who are 
  already working as creators, such as changing images to artistic styles or 
  generating images based on text. In this study, we propose a model that 
  learns brush styles of human paintings and generates new input images that 
  are redrawn using the learned brush styles. 
  The generated images by the proposed model are not perfect imitations of 
  the brush style of the reference image. This is thought to be caused by 
  the limitation of the model to generate flexible strokes due to minimal 
  stroke parameters.
  % Our model has the potential to significantly streamline the process of picture creation for creators, offering a powerful new tool for artists and designers alike.
}

\section{Introduction}
It is said that in order for people to paint a good picture, he or she needs 
to master many techniques, which takes a long time. However, with the 
advancements in machine learning, it is now possible for people without 
painting skills to generate high-quality paintings. 
For example, Midjourney\cite{Midjourney} is a software application that 
allows users to create wonderful paintings by simply entering words, 
sentences, or keywords that describe the image they wish to have painted. 
However, it is difficult to say that such a tool is useful for creators who 
have already established their own style. For creators, it is important to 
create works by their own hands, whether digital or analog, and a tool like 
Midjourney, which relies on machine learning for the entire picture creation 
process, is likely to leave creators dissatisfied with the shape and color 
of the drawn objects. Therefore, we aim to conduct research that can support 
creators during their work. Our goal is to develop a model that learns the 
brush style of a picture drawn by a creator and adapts the brush style to a 
new image. In this study, we propose a model that takes two input images, 
a content reference image and a style reference image, and generates an image 
in which what was drawn in the content reference image is redrawn with the 
brush style of the style reference image. 
Figure {fig:haru} shows examples of two input images and an output image.
The picture shown in the content reference image was created with a solid 
coloring brush, so the boundary between colors is always clear. The picture 
shown in the style reference image was created using a brush with a texture 
similar to that of an analog picture, so it is possible to make the 
borderline between colors a gradation. The generated image is a 
representation of the one drawn in the content reference image with the 
brush style of the style reference image.
t is expected that creators can greatly streamline the process of creating 
pictures by manually creating images like the content reference image shown 
in Figure\ref{fig:haru} using a simple brush, and then inputting it into the 
model to have it redrawn with the creator's brush style. Since the content 
reference image is created by the creator, the shape and color of the object 
can be controlled by the creator, and any dissatisfaction with the generated 
image can be resolved with some modifications.


\begin{figure}
  \centering
  \includegraphics[width=70mm]{resource/haru.eps}
  \caption{Examples of input and output images of the model we would like to create.}
  \label{fig:haru}
\end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[width=120mm]{resource/target_model.eps}
  \caption{Structure of the target model.}
  \label{fig:final_model}
\end{figure*}


\section{Related Works}
The use of machine learning to generate paintings has been heavily researched 
and has made significant progress. The method using neural networks as a 
generative tool is typically formulated as a pixel-wise mapping \cite{PerceptualLosses} 
or a continuous optimization process in pixel space \cite{ImageStyleTransfer}.
However, it is difficult for a neural network to imitate the process that a 
human draws. Humans typically start 
by applying rough colors and then gradually adding details, while neural 
networks generate images pixel by pixel. To imitate the human drawing 
process, the machine would have the ability to decompose a given image into 
an ordered sequence of strokes. While neural networks can create sketches 
\cite{DBLP:journals/corr/HaE17}, \cite{DBLP:journals/corr/abs-1805-00247} 
and doodles \cite{DBLP:journals/corr/abs-1810-05977} with a small number of 
strokes, creating a texture-rich painting requires many more strokes.
This makes it difficult to set loss functions for each stroke and prepare 
ground-truth strokes. As a result, Reinforcement Learning (RL) is often used 
to decompose strokes in texture-rich images. \cite{DBLP:journals/corr/abs-1804-01118, DBLP:journals/corr/abs-1206-4634, Huang_2019_ICCV}
Because the computational cost is high in the learning process of an RL agent, 
there are also studies that have formulated this process as a stroke 
parameter search. \cite{PaintTransformer}
The brush style transfers addressed in this paper are similar to those 
addressed in Neural Style Transfer \cite{ImageStyleTransfer}, in which the 
overall mood of an input style image (such as a painting) is transferred to 
another image, using CNNs. The model proposed in that study can transform a 
landscape photograph into an artistic style, but the atmosphere and colors of 
the style reference image as a whole are transferred to the content reference 
image. Therefore, characteristic objects depicted in the style reference 
image, which are not present in the content reference image, appear in the 
output image from the model. 
In this study, we aim to only transfer the style of the brush without changing 
the contents and colors drawn in the content reference image. We addressed 
this issue by adopting the Transformer-based stroke generation pipeline 
proposed by Liu \textit{et al}.\cite{PaintTransformer} as a stroke prediction 
model for our proposed model.

\section{Methodology}
The proposed model utilizes a Transformer-based framework called Paint 
Transformer proposed by Liu et al.\cite{PaintTransformer} as a stroke prediction 
pipeline. The ultimate aim of this research is to create a model that can 
redraw the content depicted in a content reference image using the brush 
style of a style reference image. To achieve this goal, it is necessary to 
alternate the optimization of stroke parameters and brush parameters.
Here, stroke parameters refer to parameters such as the color and size of 
strokes that are different for each stroke, while brush parameters refer to 
parameters common to all strokes, such as the base brush image.
However, at this stage, we have not yet implemented the double optimization.
In the following sections, we will first describe the structure of the 
target model and then present the proposed model.

\subsection{Structure of the target model}
Paint Transformer, used as a stroke prediction pipeline, is a Transformer-based 
framework. This framework generates a sequence of strokes by predicting 
stroke parameters of multiple strokes using a forward propagating transformer.
Paint Transformer consists of two modules: a stroke predictor and a stroke 
renderer. 
A stroke consists of eight parameters: center coordinates $x$, $y$, height $h$,
width $w$, rotation angle $\theta$, and $r$, $g$, $b$ corresponding to RGB values 
representing colors. Thus, stroke $s$ can be expressed as \{ $x$, $y$, $h$, $w$, $\theta$, $r$, $g$, $b$\}.
The strokes rendered by Paint Transformer are monochromatic strokes that are 
scaled and rotated from the base brush image. 
Paint Transfomer generates multiple strokes based on eight randomly generated 
parameters and uses them as a training dataset to train a stroke predictor. 
Since the training dataset is generated randomly and automatically, there is 
no need to prepare a dataset. 

The structure of the target model is shown in Figure \ref{fig:final_model}.
The upper part of the figure shows the flow of brush parameter optimization and 
the lower part shows the flow of stroke parameter optimization. First, the style 
reference image is input to Paint Transformer, and the output results are used 
to optimize the brush parameters. The optimization of brush parameters is 
performed by the following equation.

\begin{equation}
  \phi = \mathop{\rm argmin}\limits_{\phi}(\mathop{\rm argmin}\limits_{e}(L(x^{\prime}, x)) + R(e))
\end{equation}

Here, $L$ is the content loss and $R$ is the stroke regularization term. By 
inputting a style reference image $x$ as a content reference image into the 
Paint Transformer and obtaining the result $x^{\prime}$, we can determine the 
optimal stroke parameters $e$ and then use them, along with the regularization term, 
to determine the optimal brush parameters $\phi$.
Next, the content reference image is input to Paint Transformer. At this time, 
the brush parameters optimized earlier are used. The output is used to optimize 
the stroke parameters. The optimization of the stroke parameters is performed 
using the following equation.

\begin{equation}
  e = \mathop{\rm argmin}\limits_{e}(L(y^{\prime}, y) + \psi(x, y^{\prime}))
\end{equation}

Here, $L$ is the content loss and $\psi$ is the style loss. These are used to 
find the optimal stroke parameters $e$.
The above two optimizations are repeated alternately to bring the brush and 
stroke parameters closer to the optimum.

\begin{figure}[t]
  \centering
  \includegraphics[width=80mm]{resource/brushes.eps}
  \caption{Diversity of brushes developed for more accurate representation of painting effects.}
  \label{fig:brushes}
\end{figure}


\subsection{Structure of the proposed model}
As mentioned earlier, double optimization has not been implemented at this 
stage. In the proposed model, multiple base brush images are prepared, and 
the final output image is the one with the highest similarity of brush style 
with the style reference image among the results of multiple runs of Paint 
Transformer using those images. 
Paint Transformer is used for the stroke prediction model, and a function to 
evaluate the similarity of the style with the style reference image is added 
to the part after stroke prediction by Paint Transformer using the seven 
types of brush images described below.

\subsection{Brush Images}
The model proposed by Liu et al. used a single brush image to generate strokes. 
We newly created six types of brush images. Figure \ref{fig:brushes} shows 
the seven brush images. (a) is the brush image used by Liu et al. and (b)-(g) 
are the newly created ones. In (a)-(c), brush images are provided for each 
horizontal stroke and each vertical stroke.

\begin{figure}[t]
  \centering
  \includegraphics[width=80mm]{resource/dataset.eps}
  \caption{Content reference image and style reference image used in the experiment.}
  \label{fig:inputs}
\end{figure}

\subsection{Loss Function}
The $L1$ loss of the Gram matrix is employed as a loss function to evaluate 
the brush style of the output image generated by the Paint Transformer and 
the style reference image. We judged that the Gram matrix is suitable for 
the evaluation of brush styles because it allows us to consider a wide range 
of image correlations.

\begin{figure}[t]
  \centering
  \includegraphics[width=80mm]{resource/results.eps}
  \caption{Style reference image input to the model (top) and a partially enlarged image of the output from the model (bottom).}
  \label{fig:result}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=60mm]{resource/cat-starry.eps}
  \caption{Output image from Neural Style Transfer \cite{ImageStyleTransfer}.
  The content and style reference images are (a) and (c) in Figure \ref{fig:inputs}, respectively.
  }
  \label{fig:cat-starry}
\end{figure}


\begin{figure}[t]
  \centering
  \includegraphics[width=60mm]{resource/brush_discussion.eps}
  \caption{Images of the brushes used in the proposed model (left), 
  strokes drawn using Clip Studio Paint (lower right), 
  and a brush image stretched to match the strokes drawn in Clip Studio Paint (upper right).}
  \label{fig:discussion}
\end{figure}

\section{Experiments and Results}

\subsection{Experiments}
The proposed model takes a content reference image and a style reference image, 
and outputs a result image. For the content images, we used one cat image from 
The Oxford-IIIT Pet Dataset \cite{oxford-dataset} .
The content reference image and style reference image used are shown in Figure \ref{fig:inputs}.
% Image (b) is \textit{Composition with Red,
% Blue, and Yellow} by Piet Mondrian, 1930. Image (c) is \textit{The Starry Night} 
% by Vincent van Gogh, 1889.

\subsection{Results}
When the seven brush images shown in Figure \ref{fig:brushes} were used to 
generate strokes that represent the content reference image shown in Figure 
\ref{fig:inputs}, the image with the highest similarity to the style reference 
image is shown in Figure \ref{fig:result}. 
These brush styles are not very similar, which can be attributed to the fact 
that there were only seven base brush images and the parameters used to represent 
the strokes were minimal. It can be confirmed that the content of the output 
image has not changed significantly from that of the content reference image.
Image \ref{fig:cat-starry} is the output image when the content reference 
image is input to Neural Style Transfer \cite{ImageStyleTransfer} with the 
style image as (c) in Figure \ref{fig:inputs}. The brush style is imitated 
with high accuracy, but the color has changed from the original content 
reference image and the stars drawn in the style reference image have appeared.

\section{Discussions}
\subsection{Future Work}
In this study, we used a Transformer-based framework called Paint Transformer 
to redraw the content drawn on the content reference image using the brush 
style of the style reference image. In the proposed model, since the stroke 
prediction model was trained using Paint Transformer as it is, the stroke 
parameters are the same as those of Paint Transformer, i.e., five shape 
parameters and three color parameters, for a total of eight. In future studies, 
we believe that adding more parameters will enable more flexible brush stroke 
representation. The strokes generated by the proposed model are created by 
scaling and rotating the brush images shown in Figure\ref{fig:brushes}, but 
strokes can also be generated by connecting multiple brush images in 
succession. For example, CLIP STUDIO PAINT \cite{ClipStudio}, a paint software, 
can draw strokes as shown in the lower right of Figure\ref{fig:discussion}, but simply 
adjusting the size of the brush image cannot generate similar strokes, 
resulting in strokes like the one shown in the upper right of Figure\ref{fig:discussion}.
It is expected that by increasing the number of stroke parameters, we can 
increase the expressiveness of the brush and generate strokes like the one 
in the lower right of Figure\ref{fig:discussion}, which imitate the brush style of the 
style reference image. 

\subsection{Conclusion}
In this study, we proposed a novel approach to transform the content of a 
content reference image into a style that imitates the brush style of a style 
reference image using a forward propagating neural network. The final goal 
is to implement a model that iterates optimization of stroke and brush 
parameters in sequence, but the implementation of dual optimization has not 
been achieved. Although the images generated by the proposed model did not 
imitate the brush style of the style reference image, we believe that the 
implementation of the dual optimization will improve the accuracy of brush 
style imitation over the results obtained in this study.
In addition, as described in section 5.1, the small number of stroke parameters 
is also considered to be a point to be improved. Therefore, it is expected 
that the implementation of the dual optimization and improvement in the number 
of parameters will also improve the accuracy of the brush style imitation.



\begin{thebibliography}{99}

\bibitem{Midjourney}
“Midjourney,” Accessed: 2023-01-09. [Online]. Available: https://midjourney.com/.

\bibitem{PerceptualLosses}\sloppy
J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time
style transfer and super-resolution,” \textit{CoRR}, vol. abs/1603.08155, 2016.
arXiv: 1603 . 08155. [Online]. Available: http://arxiv.org/abs/1603.08155.

\bibitem{ImageStyleTransfer}
L. A. Gatys, A. S. Ecker, and M. Bethge, “Image style transfer using
convolutional neural networks,” in \textit{Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR)}, Jun. 2016.

\bibitem{DBLP:journals/corr/HaE17}\sloppy
D. Ha and D. Eck, “A neural representation of sketch drawings,” \textit{CoRR},
vol. abs/1704.03477, 2017. arXiv: 1704 . 03477. [Online]. Available:
http://arxiv.org/abs/1704.03477.

\bibitem{DBLP:journals/corr/abs-1805-00247}
J. Song, K. Pang, Y. Song, T. Xiang, and T. M. Hospedales, “Learning
to sketch with shortcut cycle consistency,” \textit{CoRR}, vol. abs/1805.00247,
2018. arXiv: 1805.00247. [Online]. Available: http://arxiv.org/abs/
1805.00247.

\bibitem{DBLP:journals/corr/abs-1810-05977}
T. Zhou, C. Fang, Z. Wang, J. Yang, B. Kim, Z. Chen, J. Brandt,
and D. Terzopoulos, “Learning to sketch with deep q networks and
demonstrated strokes,” \textit{CoRR}, vol. abs/1810.05977, 2018. arXiv: 1810.
05977. [Online]. Available: http://arxiv.org/abs/1810.05977.

\bibitem{DBLP:journals/corr/abs-1804-01118}
Y. Ganin, T. Kulkarni, I. Babuschkin, S. M. A. Eslami, and O. Vinyals,
“Synthesizing programs for images using reinforced adversarial learn
ing,” \textit{CoRR}, vol. abs/1804.01118, 2018. arXiv: 1804.01118. [Online].
Available: http://arxiv.org/abs/1804.01118.

\bibitem{DBLP:journals/corr/abs-1206-4634}
N. Xie, H. Hachiya, and M. Sugiyama, “Artist agent: A reinforcement
learning approach to automatic stroke generation in oriental ink paint
ing,” \textit{CoRR}, vol. abs/1206.4634, 2012. arXiv: 1206 . 4634. [Online].
Available: http://arxiv.org/abs/1206.4634.

\bibitem{Huang_2019_ICCV}
Z. Huang, W. Heng, and S. Zhou, “Learning to paint with model-based
deep reinforcement learning,” in \textit{Proceedings of the IEEE/CVF Inter
national Conference on Computer Vision (ICCV)}, Oct. 2019.

\bibitem{PaintTransformer}
S. Liu, T. Lin, D. He, F. Li, R. Deng, X. Li, E. Ding, and H. Wang,
“Paint transformer: Feed forward neural painting with stroke predic
tion,” in \textit{Proceedings of the IEEE International Conference on Com
puter Vision}, 2021.


\bibitem{ClipStudio}
“CLIP STUDIO PAINT,” Accessed: 2023-01-09. [Online]. Available: https://www.clipstudio.net/.

\bibitem{oxford-dataset}
O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar, “The
oxford-iiit pet dataset,” [Online]. Available: https://www.robots.ox.ac.uk/~vgg/data/pets/.

\end{thebibliography}



\end{document}
